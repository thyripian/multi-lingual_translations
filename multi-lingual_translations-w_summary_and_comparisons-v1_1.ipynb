{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73352ce",
   "metadata": {},
   "source": [
    "# <center> Multi-Lingual Translations v1.1</center>\n",
    "---\n",
    "\n",
    "###### <center>+ Summary & Comparisons </center>\n",
    "---\n",
    "<center><b>v1.0 Created by:<b></center></br>\n",
    "<center>Brandon Gromala, Sr. Data Scientist (bgromala555)</center></br>\n",
    "<center><b>v1.1 Created / Modified by:<b></center></br>\n",
    "<center>Kevan White, Sr. Data Scientist (thyripian) </center></br>\n",
    "\n",
    "<center>Release Date: 22 JUN 2023</center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca56a7",
   "metadata": {},
   "source": [
    "---\n",
    "### <center> Imports and Setup</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb016d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37900a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from pickle file\n",
    "with open('D:\\\\exports\\\\LASER\\\\spacy_sent_tokenization.pickle','rb') as file:\n",
    "    reconstructed_df = pickle.load(file)\n",
    "\n",
    "# Load the CSV data into a dataframe\n",
    "csv_df = pd.read_csv(r\"D:\\data\\embeddings\\source_data\\gdelt_20230616_bn_tl_id_20.csv\")\n",
    "\n",
    "# Make sure 'UID' columns in both dataframes are of the same type (string)\n",
    "reconstructed_df['uid'] = reconstructed_df['uid'].astype(str)\n",
    "csv_df['uid'] = csv_df['uid'].astype(str)\n",
    "\n",
    "# Merge the two dataframes on the 'uid'/'UID' column\n",
    "reconstructed_df = pd.merge(reconstructed_df, csv_df, how='inner', left_on='uid', right_on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad988060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify DataFrame was built correctly\n",
    "reconstructed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_by_languages(df, languages):\n",
    "    \"\"\"\n",
    "    Function to fetch rows from DataFrame based on the specified languages.\n",
    "    Automatically detects the language codes in the upload data, and pulls\n",
    "    all associated rows, by language.\n",
    "    \"\"\"\n",
    "    fetched_df = pd.DataFrame()\n",
    "\n",
    "    for lang in languages:\n",
    "        lang_df = df[df['meta_body_language'] == lang]\n",
    "        num_samples = len(lang_df)  # Get 5 samples or less if not available\n",
    "        if num_samples > 0:\n",
    "            sampled_lang_df = lang_df.sample(n=num_samples, random_state=1)\n",
    "            fetched_df = pd.concat([fetched_df, sampled_lang_df])\n",
    "\n",
    "    return fetched_df\n",
    "\n",
    "# Pull unique items from language column\n",
    "lang_codes = reconstructed_df['meta_body_language'].unique().tolist()\n",
    "\n",
    "# Replace NaN values with None\n",
    "lang_codes = [None if pd.isna(item) else item for item in lang_codes]\n",
    "\n",
    "# Remove None values from the list\n",
    "languages = [item for item in lang_codes if item is not None]\n",
    "\n",
    "# Call the function with the DataFrame and the list of languages\n",
    "fetched_df = fetch_data_by_languages(reconstructed_df, languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28d0cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify new DataFrame generation\n",
    "fetched_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d236f44",
   "metadata": {},
   "source": [
    "# <center>*******************************************************************************************</center>\n",
    "# <center>TRANSLATIONS</center>\n",
    "# <center>*******************************************************************************************</center>\n",
    "# Processing Option # 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c6318",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Articles NOT Reconstructed</center>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fae05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = model.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with tqdm(total=len(fetched_df),ncols=80) as pbar:\n",
    "    # Translate each sentence and add it to the 'translated_articles' column\n",
    "    for i, row in fetched_df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "\n",
    "        if isinstance(sentence, str):\n",
    "            \n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            inputs = inputs.to(device)\n",
    "            translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"])\n",
    "            translated_sentence = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "            fetched_df.loc[i, 'translated_sentences'] = translated_sentence\n",
    "        pbar.update(1)\n",
    "\n",
    "print(fetched_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b26c3",
   "metadata": {},
   "source": [
    "# <center>*******************************************************************************************</center>\n",
    "# Processing Option # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab0df6",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Articles Reconstructed</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c5880",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-1.3B\")\n",
    "model = model.to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Translate each sentence and add it to the 'translated_articles' column\n",
    "for i, row in fetched_df.iterrows():\n",
    "    article = row['sentence']\n",
    "\n",
    "    if isinstance(article, str):\n",
    "        article_sentences = nltk.sent_tokenize(article)\n",
    "        translated_article = ''\n",
    "\n",
    "        for sentence in article_sentences:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            inputs = inputs.to(device)\n",
    "            translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"])\n",
    "            translated_sentence = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "            translated_article += translated_sentence + ' '\n",
    "\n",
    "        fetched_df.loc[i, 'translated_articles'] = translated_article\n",
    "\n",
    "print(fetched_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3885e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### PLay audio alert when done processing (if tab is actively selected) #####\n",
    "\n",
    "framerate = 44100\n",
    "play_time_seconds = 1\n",
    "\n",
    "# Change these to be higher for a higher pitch\n",
    "frequency1 = 880  # was 220\n",
    "frequency2 = 884  # was 224\n",
    "\n",
    "t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "audio_data = np.sin(2*np.pi*frequency1*t) + np.sin(2*np.pi*frequency2*t)\n",
    "Audio(audio_data, rate=framerate, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f499919",
   "metadata": {},
   "source": [
    "# <center>*******************************************************************************************</center>\n",
    "# <center>END OF TRANSLATIONS</center>\n",
    "# <center>*******************************************************************************************</center>\n",
    "---\n",
    "### <center>Summary Generation</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c0ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the BART tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "def hierarchical_summarization(text):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split(\"\\n\\n\")  # Adjust the paragraph separator as per your text format\n",
    "\n",
    "    # Summarize each paragraph individually\n",
    "    paragraph_summaries = []\n",
    "    for paragraph in paragraphs:\n",
    "        # Tokenize the paragraph\n",
    "        inputs = tokenizer([paragraph], max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate the summary\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, length_penalty=2.0, max_length=150)\n",
    "        paragraph_summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        paragraph_summaries.append(paragraph_summary)\n",
    "\n",
    "    # Generate the document-level summary by combining the paragraph-level summaries\n",
    "    document_summary = \" \".join(paragraph_summaries)\n",
    "\n",
    "    return document_summary\n",
    "\n",
    "# Iterate over the DataFrame\n",
    "for i, row in fetched_df.iterrows():\n",
    "    # Get the translated article\n",
    "    translated_article = row['translated_articles']\n",
    "\n",
    "    # Generate the hierarchical summary\n",
    "    summary = hierarchical_summarization(translated_article)\n",
    "\n",
    "    # Store the summary in the DataFrame\n",
    "    fetched_df.loc[i, 'summary'] = summary\n",
    "\n",
    "print(fetched_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0c2d2",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Check Unique Values from Processing</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_items = fetched_df['translated_sentences'].unique()\n",
    "\n",
    "print(f\"Length of dataframe: {len(fetched_df['translated_sentences'])}\")\n",
    "print(f'Number of unique translations: {len(unique_items)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51688cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of non-unique translations: {len(fetched_df['translated_sentences'])-len(unique_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e2a54",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>If Completely Duplicate Rows in DataFrame, Drop Them</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01fc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetched_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada7670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cb935",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Export Data</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7682dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\exports\\\\LASER\\\\additional_langs_cleaned_translations2.pickle','wb') as file:\n",
    "    pickle.dump(fetched_df,file,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_df.to_excel('D:\\\\exports\\\\LASER\\\\new_other_langs_translations-no_summary2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443de8e",
   "metadata": {},
   "source": [
    "---\n",
    "### <center>Reconstruct Exported Data for Data Validation</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\exports\\\\LASER\\\\additional_langs_cleaned_translations2.pickle','rb') as file:\n",
    "    d1f=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19cc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
